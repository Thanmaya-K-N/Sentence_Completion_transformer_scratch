{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94aa73a0-9cc0-46ec-a83a-f6b86bc4e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and config\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb4d0e2-1652-478e-ad28-05e534d2cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load PTB\n",
    "def load_ptb_split(split=\"train\"):\n",
    "    return [line for line in PennTreebank(split=split)]\n",
    "\n",
    "train_lines = load_ptb_split(\"train\")\n",
    "val_lines = load_ptb_split(\"valid\")\n",
    "test_lines = load_ptb_split(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778193bf-a609-45d7-9c98-7f4e3993a8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9922\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Tokenizer and Vocab\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(lines):\n",
    "    for line in lines:\n",
    "        yield tokenizer(line)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_lines), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aebab9cd-535f-4347-bfa0-0fb02e16b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data tokens: 924412\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Convert text to tensor\n",
    "def process_data(lines):\n",
    "    return torch.cat([\n",
    "        torch.tensor(vocab(tokenizer(line)), dtype=torch.long)\n",
    "        for line in lines\n",
    "    ])\n",
    "\n",
    "train_data = process_data(train_lines)\n",
    "val_data = process_data(val_lines)\n",
    "test_data = process_data(test_lines)\n",
    "print(\"Train data tokens:\", train_data.size(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61065438-c660-4c71-8772-17e6ed667a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Get batches\n",
    "def get_batch(data, block_size, batch_size):\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "block_size = 64\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f0df2fe-2ee9-409b-8c3c-4a476b6c8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0b09bd-a184-4ca0-9f0c-1d36409e3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Attention function with optional masking\n",
    "def attention(q, k, v, mask=None, dropout=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = q @ k.transpose(-2, -1) / math.sqrt(d_k)  # [B, heads, T, T]\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask[:, :, :scores.size(-2), :scores.size(-1)] == 0, float('-inf'))\n",
    "\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        attn = dropout(attn)\n",
    "\n",
    "    return attn @ v  # [B, heads, T, d_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5815e243-18b7-404f-81ce-068923b4b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "    \n",
    "        # Project to Q, K, V and reshape\n",
    "        qkv = self.qkv_proj(x)  # (B, T, 3*C)\n",
    "        qkv = qkv.view(B, T, 3, self.num_heads, self.d_k)  # (B, T, 3, heads, d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, T, d_k)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: (B, heads, T, d_k)\n",
    "    \n",
    "        # Attention\n",
    "        x = attention(q, k, v, mask=mask, dropout=self.dropout)  # (B, heads, T, d_k)\n",
    "    \n",
    "        # Recombine heads\n",
    "        x = x.transpose(1, 2).contiguous()  # (B, T, heads, d_k)\n",
    "        x = x.view(B, T, C)  # (B, T, C)\n",
    "    \n",
    "        return self.out_proj(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1539fe49-a24b-4794-9d4c-dc5bacab4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Position-wise feedforward network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f05b48e-f3ad-4c34-bb67-ee610e57221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: One decoder block\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.ff = FeedForward(embed_dim, ff_dim, dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.ln1(x), mask)\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ada0aa9-b588-49ae-ae54-7eb678656069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Full GPT model\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, num_layers, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len=block_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size, \"Sequence length exceeds block size.\"\n",
    "\n",
    "        tok_emb = self.token_embed(idx)           # (B, T, C)\n",
    "        x = self.pos_enc(tok_emb)                 # (B, T, C)\n",
    "        mask = torch.tril(torch.ones(T, T)).to(device)\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, T, T]\n",
    "\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=mask)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)                     # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ade8b98c-0db2-4b27-a205-af7974a35201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Define hyperparameters and initialize model\n",
    "embed_dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "ff_dim = 1024\n",
    "dropout = 0.1\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    block_size=block_size,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "761b9230-8777-4b0b-aeec-823714442881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Training loop\n",
    "def train(model, train_data, val_data, epochs=5, batch_size=32, block_size=64, eval_interval=500):\n",
    "    model.train()\n",
    "    steps_per_epoch = 1400\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for step in range(steps_per_epoch):\n",
    "            xb, yb = get_batch(train_data, block_size, batch_size)\n",
    "\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if step % eval_interval == 0 and step > 0:\n",
    "                val_loss = evaluate(model, val_data, block_size, batch_size)\n",
    "                print(f\"Epoch {epoch+1}, Step {step}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / step\n",
    "        print(f\"Epoch {epoch+1} completed | Avg Train Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7f2551c-0135-44ce-9f3a-450c9ce8281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for _ in range(50):  # evaluate on 50 mini-batches\n",
    "        xb, yb = get_batch(data, block_size, batch_size)\n",
    "        _, loss = model(xb, yb)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "308a1c4b-e896-4b6c-a4fd-52db1d3a91ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 200, Train Loss: 4.4192, Val Loss: 4.7861\n",
      "Epoch 1, Step 400, Train Loss: 4.2175, Val Loss: 4.7714\n",
      "Epoch 1, Step 600, Train Loss: 4.4711, Val Loss: 4.7919\n",
      "Epoch 1, Step 800, Train Loss: 4.2877, Val Loss: 4.7762\n",
      "Epoch 1, Step 1000, Train Loss: 4.0098, Val Loss: 4.7211\n",
      "Epoch 1, Step 1200, Train Loss: 4.0254, Val Loss: 4.7668\n",
      "Epoch 1 completed | Avg Train Loss: 4.1910\n",
      "Epoch 2, Step 200, Train Loss: 4.2112, Val Loss: 4.6928\n",
      "Epoch 2, Step 400, Train Loss: 3.8234, Val Loss: 4.7334\n",
      "Epoch 2, Step 600, Train Loss: 3.8045, Val Loss: 4.7407\n",
      "Epoch 2, Step 800, Train Loss: 3.8953, Val Loss: 4.7343\n",
      "Epoch 2, Step 1000, Train Loss: 4.1481, Val Loss: 4.7125\n",
      "Epoch 2, Step 1200, Train Loss: 3.9769, Val Loss: 4.7365\n",
      "Epoch 2 completed | Avg Train Loss: 3.9385\n",
      "Epoch 3, Step 200, Train Loss: 3.8361, Val Loss: 4.7048\n",
      "Epoch 3, Step 400, Train Loss: 3.8071, Val Loss: 4.7656\n",
      "Epoch 3, Step 600, Train Loss: 3.6637, Val Loss: 4.7661\n",
      "Epoch 3, Step 800, Train Loss: 3.4489, Val Loss: 4.7519\n",
      "Epoch 3, Step 1000, Train Loss: 3.4598, Val Loss: 4.7560\n",
      "Epoch 3, Step 1200, Train Loss: 3.6563, Val Loss: 4.7659\n",
      "Epoch 3 completed | Avg Train Loss: 3.7203\n",
      "Epoch 4, Step 200, Train Loss: 3.7036, Val Loss: 4.8411\n",
      "Epoch 4, Step 400, Train Loss: 3.6209, Val Loss: 4.8141\n",
      "Epoch 4, Step 600, Train Loss: 3.3415, Val Loss: 4.7911\n",
      "Epoch 4, Step 800, Train Loss: 3.4741, Val Loss: 4.8700\n",
      "Epoch 4, Step 1000, Train Loss: 3.4998, Val Loss: 4.8219\n",
      "Epoch 4, Step 1200, Train Loss: 3.4371, Val Loss: 4.8592\n",
      "Epoch 4 completed | Avg Train Loss: 3.5255\n",
      "Epoch 5, Step 200, Train Loss: 3.5228, Val Loss: 4.8438\n",
      "Epoch 5, Step 400, Train Loss: 3.3328, Val Loss: 4.8445\n",
      "Epoch 5, Step 600, Train Loss: 3.3835, Val Loss: 4.8786\n",
      "Epoch 5, Step 800, Train Loss: 3.3841, Val Loss: 4.9102\n",
      "Epoch 5, Step 1000, Train Loss: 3.1918, Val Loss: 4.9098\n",
      "Epoch 5, Step 1200, Train Loss: 3.3406, Val Loss: 4.8845\n",
      "Epoch 5 completed | Avg Train Loss: 3.3467\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Run training\n",
    "train(\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    epochs=5,\n",
    "    batch_size=batch_size,\n",
    "    block_size=block_size,\n",
    "    eval_interval=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9767167a-4c9b-401c-bbe0-22f610658202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss: 4.8508\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Final evaluation on test set\n",
    "test_loss = evaluate(model, test_data, block_size, batch_size)\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "047f4546-a362-45a4-8ee2-aba4ba3b5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Text generation function\n",
    "def generate_text(model, vocab, tokenizer, prompt, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(vocab(tokenizer(prompt)), dtype=torch.long).unsqueeze(0).to(device)  # [1, T]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -block_size:]  # crop to block_size\n",
    "        logits, _ = model(idx_cond)\n",
    "        next_token_logits = logits[:, -1, :]  # [1, vocab_size]\n",
    "        probs = F.softmax(next_token_logits, dim=-1)  # [1, vocab_size]\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # [1, 1]\n",
    "        idx = torch.cat((idx, next_token), dim=1)  # append to sequence\n",
    "\n",
    "    output_tokens = idx[0].tolist()\n",
    "    output_text = ' '.join(vocab.lookup_tokens(output_tokens))\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53926c87-fde3-4cba-88eb-78246ee375cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "\n",
      "in the future <unk> funds have fallen to several percentage points on the decline in yields on the three-month cds the derivative markets fell higher than the the average yield on six-month treasury bills was slightly stronger says <unk> <unk> a vice president for finance at credit suisse the show <unk> several health disasters\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Run generation from a prompt\n",
    "prompt = \"In the future,\"\n",
    "generated = generate_text(model, vocab, tokenizer, prompt, max_new_tokens=50)\n",
    "print(\"Generated text:\\n\")\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "270e0339-6ea5-4757-89f8-81fe088027c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gpt_ptb_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Save trained model\n",
    "model_path = \"gpt_ptb_model.pth\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"vocab\": vocab,\n",
    "    \"config\": {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"block_size\": block_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"ff_dim\": ff_dim,\n",
    "        \"dropout\": dropout\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd540fe9-e908-484b-8f39-d5d906b00865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
